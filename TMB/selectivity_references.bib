@article{Cheng2023,
abstract = {Many demographic processes vary by age and over time, but are also hypothesized to exhibit cohort-specific patterns in variation; accounting for this variation within fisheries management remains a key challenge for contemporary stock assessments. Although there is evidence for time, age, and cohort-specific patterns in the variation of various components within stock assessment (e.g., selectivity, growth), methods are sparsely documented or are lacking to simultaneously estimate autocorrelation over time, among ages, and by cohort while also quantifying residual variation. We demonstrate an approach that facilitates the simultaneous estimation of autocorrelation for time, age, and cohort correlations, and provide two options to estimate the pointwise variance of this process (termed conditional and marginal variance). Using eastern Bering Sea walleye pollock (Gadus chalcogrammus) as a case-study, we develop factorial model formulations to demonstrate differences in predicted weight-at-age values from models that estimate different combinations of correlation parameters along three axes (age, year, cohort). We show that traditional model selection tools can be used to identify the relative evidence for, and magnitude of, age, time, and cohort correlations, and demonstrate that this method can be easily integrated as a routine option within next-generation stock assessments. Code to replicate our analysis is provided in a GitHub repository (https://github.com/chengmatt/GMRF_WAA), and includes a Template Model Builder function for assembling the precision matrix to facilitate easy adoption in other software packages.},
author = {Cheng, Matthew LH and Thorson, James T. and Ianelli, James N. and Cunningham, Curry J.},
doi = {10.1016/j.fishres.2023.106755},
file = {:C\:/Users/Grant Adams/Documents/Mendeley Desktop/Cheng et al. - 2023 - Unlocking the triad of age, year, and cohort effects for stock assessment Demonstration of a computationally effic.pdf:pdf},
issn = {01657836},
journal = {Fisheries Research},
keywords = {Age-and time-varying,Cohort effects,Mixed-effects,State-space,Stock assessment},
number = {June},
pages = {106755},
publisher = {Elsevier B.V.},
title = {{Unlocking the triad of age, year, and cohort effects for stock assessment: Demonstration of a computationally efficient and reproducible framework using weight-at-age}},
url = {https://doi.org/10.1016/j.fishres.2023.106755},
volume = {266},
year = {2023}
}
@article{Fisch2023,
abstract = {Increasingly, mixed-effect fishery stock assessment models are being developed where deviations about functional forms of different processes are modelled as random effects and the extent of variance is estimated internal to the model. Concurrently, sampling variance parameters associated with likelihoods for fitting composition data within fisheries assessments are more often being estimated internal to the model as well. We examine the performance of stock assessment models when multiple process variance and sampling variance terms are simultaneously estimated within assessment models. We specifically examine how assessment performance is affected by the choice of composition likelihood, the degree of overdispersion in composition data, overparameterization, and modelling variation on the wrong process. In doing so, we build a simulation containing overdispersion and correlations in composition data, directional variation in catchability and/or selectivity, and estimation models which include random effects and composition likelihoods with theoretically estimable variances. Results suggest that with standard data available in fisheries assessments, process variance parameters associated with some commonly employed methods and sampling variance parameters can be simultaneously estimated internal to an assessment, and performance greatly improves with increased composition data. Our results also suggest little downside to overparameterization of selectivity and catchability when the true process is not time-varying, which largely agrees with previous research. However, when a process is truly time-varying and the assessment models time-variation on a different process, namely when selectivity is time-varying and instead natural mortality is modelled as potentially time-varying, we find a risk of severe increases in bias and decreases in confidence interval coverage for assessed quantities. This bias and decrease in coverage could, however, be partially mitigated by also modelling time-variation on the correct process.},
author = {Fisch, N and Shertzer, K and Camp, E and Maunder, M and Ahrens, R},
doi = {10.1093/icesjms/fsad138},
file = {:C\:/Users/Grant Adams/Documents/Mendeley Desktop/Fisch et al. - 2023 - Process and sampling variance within fisheries stock assessment models estimability, likelihood choice, and the co.pdf:pdf},
issn = {1054-3139},
journal = {ICES Journal of Marine Science},
keywords = {age-structured models,composition data,mixed-effects,overdispersion,process error,process variation,sampling error,stock assessment},
number = {September},
pages = {2125--2149},
title = {{Process and sampling variance within fisheries stock assessment models: estimability, likelihood choice, and the consequences of incorrect specification}},
year = {2023}
}
@article{Maunder2013,
abstract = {Limited data, and the requirement to provide science-based advice for exploited populations, have led to the development of statistical methods that combine several sources of information into a single analysis. This approach, " integrated analysis" was first formulated by Fournier and Archibald in 1982. Contemporary use of integrated analysis involves using all available data, in as raw a form as appropriate, in a single analysis. Analyses that were traditionally carried out independently are now conducted simultaneously through likelihood functions that include multiple data sources. For example, the traditional analysis of converting catch-at-length data into catch-at-age data for use in an age-structured population dynamics models can be avoided by including the basic data used in this conversion, length-frequency and conditional age-at-length data, in the likelihood function. This allows for consistency in assumptions and permits the uncertainty associated with both data sources to be propagated to final model outputs, such as catch limits under harvest control rules. The development of the AD Model Builder software has greatly facilitated the use of integrated analyses, and there are now several general stock assessment models (e.g., Stock Synthesis) that allow many data types and model assumptions to be analyzed simultaneously. In this paper, we define integrated analysis, describe its history and development, give several examples, and describe the advantages of and problems with integrated analysis. {\textcopyright} 2012 Elsevier B.V.},
author = {Maunder, Mark N. and Punt, Andr{\'{e}} E.},
doi = {10.1016/j.fishres.2012.07.025},
file = {:C\:/Users/Grant Adams/Documents/Mendeley Desktop/Maunder, Punt - 2013 - A review of integrated analysis in fisheries stock assessment(3).pdf:pdf},
issn = {01657836},
journal = {Fisheries Research},
keywords = {Bayesian,Data assimilation,Fisheries stock assessment,Integrated analysis,Maximum likelihood,Meta-analysis,Multivariate nonlinear regression},
pages = {61--74},
publisher = {Elsevier B.V.},
title = {{A review of integrated analysis in fisheries stock assessment}},
url = {http://dx.doi.org/10.1016/j.fishres.2012.07.025},
volume = {142},
year = {2013}
}
@article{Nielsen2014,
abstract = {Time-varying selectivity is one of the main challenges in single species age-based assessment models. In classical deterministic VPA-type models the fishing mortality rates are unfiltered representations of the observed catches. As a consequence the selectivity becomes time-varying, but this representation is too fluctuating, because it includes the observation noise. In parametric statistical catch at age models a common assumption is that the selectivity is constant in all years, although time-varying selectivity can be introduced by splitting the data period in blocks with different selectivities, or by using smoothing splines and penalized time-deviances. However, these methods require subjective choices w.r.t. the degree of time-varying allowed. A simple state-space assessment model is presented as an alternative, which among other benefits offers an objective way of estimating time-varying selectivity pattern. The fishing mortality rates are considered (possibly correlated) stochastic processes, and the corresponding process variances are estimated within the model. The model is applied to North Sea cod and it is verified from simulations that time-varying selectivity can be estimated. {\textcopyright} 2014 Elsevier B.V.},
author = {Nielsen, Anders and Berg, Casper W.},
doi = {10.1016/j.fishres.2014.01.014},
file = {:C\:/Users/Grant Adams/Documents/Mendeley Desktop/Nielsen, Berg - 2014 - Estimation of time-varying selectivity in stock assessments using state-space models.pdf:pdf},
isbn = {01657836},
issn = {01657836},
journal = {Fisheries Research},
keywords = {Catch-at-age analysis,Selectivity,State-space models,Stock assessment},
pages = {96--101},
publisher = {Elsevier B.V.},
title = {{Estimation of time-varying selectivity in stock assessments using state-space models}},
url = {http://dx.doi.org/10.1016/j.fishres.2014.01.014},
volume = {158},
year = {2014}
}
@article{Privitera-Johnson2022,
abstract = {Specification of how selectivity (the combination of availability and vulnerability) is modelled in integrated stock assessments is key to avoiding bias in estimates of quantities of management interest. Many “rules of thumb” are common in the community but these have yet to be rigorously tested. This paper uses simulation to compare 12 approaches for specifying selectivity in an age-structured integrated stock assessment, including parametric and non-parametric approaches. The operating model represents a two-fishery case where selectivity for one or both fisheries can be dome-shaped and/or time-varying. The results suggest that using AIC to select among selectivity forms is not robust, including when model misspecification is absent, even though the use of model selection criteria such as AIC is common when conducting stock assessments. The use of double normal selectivity was found to be most robust to uncertainty in the true form of selectivity. Estimation of time-variation in selectivity did not lead to appreciable improvements in performance when the true time-variation was random. The double normal form performed poorly if M was estimated along with the other model parameters. Similarly, use of flexible parametric methods, such as splines, performed adequately with informative data, but poorly when the catch series exhibited low contrast and age-composition data were not available from the start of the fishery. This suggests that the best practices for selectivity will depend on knowledge of the likely information content of the data.},
author = {Privitera-Johnson, Kristin M. and Methot, Richard D. and Punt, Andr{\'{e}} E.},
doi = {10.1016/j.fishres.2022.106247},
file = {:C\:/Users/Grant Adams/Documents/Mendeley Desktop/Privitera-Johnson, Methot, Punt - 2022 - Towards best practice for specifying selectivity in age-structured integrated stock assessments.pdf:pdf},
issn = {01657836},
journal = {Fisheries Research},
keywords = {Best practice, non-parametric selectivity,Simulation,Stock assessment,Stock synthesis},
number = {January},
title = {{Towards best practice for specifying selectivity in age-structured integrated stock assessments}},
volume = {249},
year = {2022}
}
@article{Punt2023,
abstract = {The ideal stock assessment would be able to estimate all of the key parameters related to population processes within a framework that assigns appropriate weight to the data, fits the data adequately, and captures all sources of uncertainty related to estimation, including model uncertainty, process uncertainty, and observation uncertainty. The aim of good practice guidelines is to avoid the pitfalls of earlier analysis methods, and consequently provide assessments that reflect objective scientific information on which management decisions can be based. This paper outlines a framework for the component of a stock assessment related to fitting population dynamics models to monitoring data to support decision making, which follows from what would be considered good (but not necessarily best) practice in the field. The paper identifies current good and best practices related to selecting a model structure, parameterizing growth, recruitment, natural mortality and the stock-recruitment relationship, as well as how to select among model configurations based on diagnostics and weight data and priors within assessments based on the existing literature, including past Center for the Advancement of Population Assessment Methodology (CAPAM) workshop reports and the results of simulation studies that explored the performances of different ways to configure stock assessments.},
author = {Punt, Andr{\'{e}} E.},
doi = {10.1016/j.fishres.2023.106642},
file = {:C\:/Users/Grant Adams/Documents/Mendeley Desktop/Punt - 2023 - Those who fail to learn from history are condemned to repeat it A perspective on current stock assessment good practices a.pdf:pdf},
issn = {01657836},
journal = {Fisheries Research},
keywords = {Fisheries assessment,Integrated analysis,Management,Uncertainty},
number = {January},
title = {{Those who fail to learn from history are condemned to repeat it: A perspective on current stock assessment good practices and the consequences of not following them}},
volume = {261},
year = {2023}
}
@article{Punt2014,
abstract = {The choice of how to model selectivity differs among approaches to fisheries stock assessment. VPA tends to make only weak assumptions regarding (age-specific) selectivity (asymptotic selectivity and temporal stability of selectivity for the most recent years). In contrast, selectivity is more parametric in "integrated" methods, and can be age-, length-, and age- and length-based. The use of parametric selectivity functions tends to reduce estimation variation because fewer parameters have to be estimated, but incorrect choices for the functional form for selectivity can lead to bias. This paper illustrates effects of poor choices for selectivity on the outcomes of stock assessments, outlines methods for evaluating whether a particular choice for selectivity is appropriate using residual diagnostics, and summarizes current ways to select among alternative functional forms for selectivity. This paper also provides a synthesis of the results of past simulation studies which have explored the ability to correctly parameterize selectivity. {\textcopyright} 2013 Elsevier B.V.},
author = {Punt, Andr{\'{e}} E. and Hurtado-Ferro, Felipe and Whitten, Athol R.},
doi = {10.1016/j.fishres.2013.06.003},
file = {:C\:/Users/Grant Adams/Documents/Mendeley Desktop/Punt, Hurtado-Ferro, Whitten - 2014 - Model selection for selectivity in fisheries stock assessments(3).pdf:pdf},
issn = {01657836},
journal = {Fisheries Research},
keywords = {AIC,Asymptotic selectivity,Dome-shaped selectivity,Information criteria model selection,Simulation},
pages = {124--134},
publisher = {Elsevier B.V.},
title = {{Model selection for selectivity in fisheries stock assessments}},
url = {http://dx.doi.org/10.1016/j.fishres.2013.06.003},
volume = {158},
year = {2014}
}
@article{Skaug2006,
abstract = {Fitting of non-Gaussian hierarchical random effects models by approximate maximum likelihood can be made automatic to the same extent that Bayesian model fitting can be automated by the program BUGS. The word "automatic" means that the technical details of computation are made transparent to the user. This is achieved by combining a technique from computer science known as "automatic differentiation" with the Laplace approximation for calculating the marginal likelihood. Automatic differentiation, which should not be confused with symbolic differentiation, is mostly unknown to statisticians, and hence basic ideas and results are reviewed. The computational performance of the approach is compared to that of existing mixed-model software on a suite of datasets selected from the mixed-model literature. {\textcopyright} 2006 Elsevier B.V. All rights reserved.},
author = {Skaug, Hans J. and Fournier, David A.},
doi = {10.1016/j.csda.2006.03.005},
file = {:C\:/Users/Grant Adams/Documents/Mendeley Desktop/Skaug, Fournier - 2006 - Automatic approximation of the marginal likelihood in non-Gaussian hierarchical models.pdf:pdf},
issn = {01679473},
journal = {Computational Statistics and Data Analysis},
keywords = {AD Model Builder,Automatic differentiation,Importance sampling,Laplace approximation,Mixed models,Random effects},
number = {2},
pages = {699--709},
title = {{Automatic approximation of the marginal likelihood in non-Gaussian hierarchical models}},
volume = {51},
year = {2006}
}
@article{Stock2021,
abstract = {The rapid changes observed in many marine ecosystems that support fisheries pose a challenge to stock assessment and management predicated on time-invariant productivity and considering species in isolation. In single-species assessments, two main approaches have been used to account for productivity changes: allowing biological parameters to vary stochastically over time (empirical), or explicitly linking population processes such as recruitment (R) or natural mortality (M) to environmental covariates (mechanistic). Here, we describe the Woods Hole Assessment Model (WHAM) framework and software package, which combines these two approaches. WHAM can estimate time- and age-varying random effects on annual transitions in numbers at age (NAA), M, and selectivity, as well as fit environmental time-series with process and observation errors, missing data, and nonlinear links to R and M. WHAM can also be configured as a traditional statistical catch-at-age (SCAA) model in order to easily bridge from status quo models and test them against models with state-space and environmental effects, all within a single framework. We fit models with and without (independent or autocorrelated) random effects on NAA, M, and selectivity to data from five stocks with a broad range of life history, fishing pressure, number of ages, and time-series length. Models that included random effects performed well across stocks and processes, especially random effects models with a two dimensional (2D) first-order autoregressive, AR(1), covariance structure over age and year. We conducted simulation tests and found negligible or no bias in estimation of important assessment outputs (SSB, F, stock status, and catch) when the operating and estimation models matched. However, bias in SSB and F was often non-trivial when the estimation model was less complex than the operating model, especially when models without random effects were fit to data simulated from models with random effects. Bias of the variance and correlation parameters controlling random effects was also negligible or slightly negative as expected. Our results suggest that WHAM can be a useful tool for stock assessment when environmental effects on R or M, or stochastic variation in NAA transitions, M, or selectivity are of interest. In the U.S. Northeast, where the productivity of several groundfish stocks has declined, conducting assessments in WHAM with time-varying processes via random effects or environment-productivity links may account for these trends and potentially reduce retrospective bias.},
author = {Stock, Brian C. and Miller, Timothy J.},
doi = {10.1016/j.fishres.2021.105967},
file = {:C\:/Users/Grant Adams/Documents/Mendeley Desktop/Stock, Miller - 2021 - The Woods Hole Assessment Model (WHAM) A general state-space assessment framework that incorporates time- and age.pdf:pdf},
issn = {01657836},
journal = {Fisheries Research},
keywords = {Environmental effects,Natural mortality,Random effects,Recruitment,State-space,Stock assessment,Template Model Builder (TMB),Time-varying},
number = {April},
pages = {105967},
publisher = {Elsevier B.V.},
title = {{The Woods Hole Assessment Model (WHAM): A general state-space assessment framework that incorporates time- and age-varying processes via random effects and links to environmental covariates}},
url = {https://doi.org/10.1016/j.fishres.2021.105967},
volume = {240},
year = {2021}
}
@article{Szuwalski2018,
abstract = {Retrospective patterns are consistent directional changes in assessment estimates of biomass in a given year when additional years of data are added to an assessment, and have been identified for a number of exploited marine stocks. Retrospective patterns are sometimes reduced by allowing population processes to vary over time in an assessment, but it is unclear how this practice influences management performance. We simulated stocks in which retrospective patterns were induced by forcing natural mortality, selectivity, or growth to vary over time. We then evaluated the impacts of reducing retrospective patterns by allowing population processes to vary in the assessment. In general, allowing selectivity, natural mortality, and growth to vary in the assessment decreased the magnitude of retrospective patterns in estimated spawning biomass, regardless of whether the true time-varying process was allowed to vary. However, the resulting reference points and management advice were sometimes drastically in error when a process other than the true time-varying process was allowed to vary, and these errors resulted in under-utilizing or over-exploiting the stock. Given the potential for error, identifying the important population processes that vary over time when addressing retrospective patterns should be a priority when providing management advice and may require increased longitudinal life history studies.},
author = {Szuwalski, Cody S. and Ianelli, James N. and Punt, Andr{\'{e}} E.},
doi = {10.1093/icesjms/fsx159},
file = {:C\:/Users/Grant Adams/Documents/Mendeley Desktop/Szuwalski, Ianelli, Punt - 2018 - Reducing retrospective patterns in stock assessment and impacts on management performance(2).pdf:pdf},
issn = {10959289},
journal = {ICES Journal of Marine Science},
keywords = {climate change,management strategy evaluation,retrospective bias,retrospective pattern,stock assessment},
number = {2},
pages = {596--609},
title = {{Reducing retrospective patterns in stock assessment and impacts on management performance}},
volume = {75},
year = {2018}
}
@article{Trijoulet2023,
abstract = {Stock assessment models are often used to inform fisheries management and need therefore to be thoroughly validated. Different diagnostics exist to validate models including the analysis of standardized residuals. Standardized residuals are commonly calculated by subtracting prediction from the observation and dividing the result with the estimated standard deviation (i.e., Pearson residuals). Many currently applied stock assessment models fit to compositional observations (e.g., age, length or stock compositions) using multivariate distributions. These distributions create correlation between observations, which are propagated in the residuals if estimated as Pearson. This study shows that using Pearson residuals to analyze goodness of the fit, when data are fitted using a multivariate distribution, is incorrect and one-step-ahead (OSA) or forecast quantile residuals should be used instead. For such distributions, OSA residuals are independent and standard normally distributed for correctly specified models. This study describes the calculation of OSA residuals specifically to de-correlate compositional observations for the multivariate distributions most commonly used in assessment models. This allows composition observations to be evaluated with the same statistical rigor as residuals from uncorrelated observations. This also prevents the possible wrong interpretation of Pearson residuals and the rejection of a correct model. We have developed an R-package that estimates OSA residuals externally to the model for models that do not include random processes. For models that use random processes, the distributions are now developed in Template Model Builder and explained in detail here for internal use.},
author = {Trijoulet, Vanessa and Albertsen, Christoffer Moesgaard and Kristensen, Kasper and Legault, Christopher M. and Miller, Timothy J. and Nielsen, Anders},
doi = {10.1016/j.fishres.2022.106487},
file = {:C\:/Users/Grant Adams/Documents/Mendeley Desktop/Trijoulet et al. - 2023 - Model validation for compositional data in stock assessment models Calculating residuals with correct properti.pdf:pdf},
issn = {01657836},
journal = {Fisheries Research},
keywords = {Multivariate distributions,One-step-ahead quantile residuals,Pearson,Template Model Builder,compResidual R-package},
number = {July 2022},
pages = {106487},
publisher = {Elsevier B.V.},
title = {{Model validation for compositional data in stock assessment models: Calculating residuals with correct properties}},
url = {https://doi.org/10.1016/j.fishres.2022.106487},
volume = {257},
year = {2023}
}
@article{Xu2019,
abstract = {Selectivity is a key parameter in stock assessments that describes how fisheries interact with different ages and sizes of fish. It is usually confounded with other processes (e.g., natural mortality and recruitment) in stock assessments and the assumption of selectivity can strongly affect stock assessment outcome. Here, we introduce a new semi-parametric selectivity method, which we implement and test in Stock Synthesis. This selectivity method includes a parametric component and an autocorrelated nonparametric component consisting of deviations from the parametric component. We explore the new selectivity method using two simulation experiments, which show that the two autocorrelation parameters for selectivity deviations of data-rich fisheries are estimable using either mixed-effect or simpler sample-based algorithms. When selectivity deviations of a data-rich fishery are highly autocorrelated, using the new method to estimate the two autocorrelation parameters leads to more precise estimations of spawning biomass and fully selected fishing mortality. However, this new method fails to improve model performance in low data quality cases where measurement error in the data overwhelms the pattern caused by the autocorrelated process. Finally, we use a case study involving North Sea herring (Clupea harengus) to show that our new method substantially reduces autocorrelations in the Pearson residuals in fit to age composition data.},
author = {Xu, Haikun and Thorson, James T. and Methot, Richard D. and Taylor, Ian G.},
doi = {10.1139/cjfas-2017-0446},
file = {:C\:/Users/Grant Adams/Documents/Mendeley Desktop/Xu et al. - 2019 - A new semi-parametric method for autocorrelated age-and time-varying selectivity in age-structured assessment models.pdf:pdf},
issn = {12057533},
journal = {Canadian Journal of Fisheries and Aquatic Sciences},
number = {2},
pages = {268--285},
title = {{A new semi-parametric method for autocorrelated age-and time-varying selectivity in age-structured assessment models}},
volume = {76},
year = {2019}
}
@article{Kristensen2015,
abstract = {TMB is an open source R package that enables quick implementation of complex nonlinear random effect (latent variable) models in a manner similar to the established AD Model Builder package (ADMB, admb-project.org). In addition, it offers easy access to parallel computations. The user defines the joint likelihood for the data and the random effects as a C++ template function, while all the other operations are done in R; e.g., reading in the data. The package evaluates and maximizes the Laplace approximation of the marginal likelihood where the random effects are automatically integrated out. This approximation, and its derivatives, are obtained using automatic differentiation (up to order three) of the joint likelihood. The computations are designed to be fast for problems with many random effects ($\sim$10^6) and parameters ($\sim$10^3). Computation times using ADMB and TMB are compared on a suite of examples ranging from simple models to large spatial models where the random effects are a Gaussian random field. Speedups ranging from 1.5 to about 100 are obtained with increasing gains for large problems. The package and examples are available at http://tmb-project.org.},
archivePrefix = {arXiv},
arxivId = {1509.00660},
author = {Kristensen, Kasper and Nielsen, Anders and Berg, Casper W and Skaug, Hans and Bell, Bradley M.},
doi = {10.18637/jss.v070.i05},
eprint = {1509.00660},
file = {:C\:/Users/Grant Adams/Documents/Mendeley Desktop/Kristensen et al. - 2015 - TMB Automatic Differentiation and Laplace Approximation.pdf:pdf},
issn = {1548-7660},
journal = {Journal of Statistical Software},
keywords = {ad,automatic differentiation,c,latent variables,random effects,templates},
number = {5},
pages = {1--21},
title = {{TMB : Automatic Differentiation and Laplace Approximation}},
url = {http://www.jstatsoft.org/v70/i05/},
volume = {70},
year = {2016}
}
